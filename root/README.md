<!-- # Comparacao-de-tecnologias-padroes-e-protocolos-em-sistemas-distribuidos -->

<!-- Colocar fururamente como rodar ambiente de desenvolvimento para todos os "casos"


Ferramentas

-DrawIO para diagramas



O propÃ³sito do projeto Ã© realizar uma anÃ¡lise comparativa entre seis tecnologias de comunicaÃ§Ã£o em sistemas distribuÃ­dos â€” REST, SOAP, gRPC, WebSocket, GraphQL e Webhook â€” avaliando suas caracterÃ­sticas, pontos fortes e limitaÃ§Ãµes por meio de experimentos controlados. Para isso, o trabalho foi dividido em quatro etapas bem definidas: levantamento teÃ³rico, planejamento experimental, desenvolvimento e testes. O levantamento teÃ³rico consolidou os conceitos fundamentais sobre protocolos e padrÃµes, suas aplicaÃ§Ãµes tÃ­picas e as razÃµes que justificam incluÃ­-los no estudo; ele serviu de base para a definiÃ§Ã£o dos serviÃ§os, das mÃ©tricas e dos cenÃ¡rios experimentais que compÃµem o nÃºcleo do planejamento. O planejamento experimental descreve os serviÃ§os que seriam implementados, os cenÃ¡rios de teste (ideal, realista e crÃ­tico), as mÃ©tricas a serem coletadas, a instrumentaÃ§Ã£o necessÃ¡ria e os diagramas que documentam a soluÃ§Ã£o. A fase de desenvolvimento consistiu na implementaÃ§Ã£o prÃ¡tica dos serviÃ§os nos dois ambientes escolhidos: Python, responsabilidade do Vitor, e Java, responsabilidade do Sanzio. Finalmente, a fase de testes conduziu experimentos com ferramentas de carga e monitoramento para obter dados quantitativos que deram suporte Ã s conclusÃµes.

O experimento foi desenhado para ser reprodutÃ­vel e comparÃ¡vel. Para isso foram escolhidos trÃªs microserviÃ§os que representam cenÃ¡rios de uso distintos. O primeiro, chamado user-service, simula um serviÃ§o CRUD simples para manipulaÃ§Ã£o de usuÃ¡rios e serve como referÃªncia para requisiÃ§Ãµes pontuais e leitura/escrita tÃ­picas de APIs. O segundo, chamado message-service, simula comunicaÃ§Ãµes mais orientadas a mensagens e tempo real, representando casos em que hÃ¡ troca contÃ­nua de dados ou notificaÃ§Ãµes frequentes. O terceiro, event-service, foi pensado para cenÃ¡rios orientados a eventos e integraÃ§Ãµes assÃ­ncronas, especificamente para testar webhooks e entregas de eventos. Cada um desses serviÃ§os foi implementado nas linguagens definidas, de modo que existam versÃµes em Python e em Java quando aplicÃ¡vel, e todos seguem um contrato funcional mÃ­nimo padronizado para permitir comparaÃ§Ãµes diretas entre protocolos. O contrato padronizado define a mesma entidade base, User, com os campos id, name e email, e operaÃ§Ãµes mÃ­nimas como obter usuÃ¡rio por id e criar usuÃ¡rio. As definiÃ§Ãµes concretas foram documentadas no arquivo docs/api_contracts.md, que contÃ©m exemplos de endpoints REST, WSDL e operaÃ§Ãµes SOAP, o arquivo proto para gRPC, queries e mutations do GraphQL, o formato de mensagens para WebSocket e a estrutura de eventos e callbacks para Webhook.

A infraestrutura experimental foi construÃ­da com contÃªineres Docker para assegurar isolamento, portabilidade e reprodutibilidade dos testes. Todos os serviÃ§os sÃ£o empacotados com Dockerfiles e orquestrados por um docker-compose que contÃ©m os serviÃ§os de aplicaÃ§Ã£o e a infraestrutura de observabilidade. A camada de monitoramento foi composta por Prometheus para coleta de mÃ©tricas e Grafana para visualizaÃ§Ã£o, com os serviÃ§os instrumentados para expor mÃ©tricas compatÃ­veis com Prometheus. No caso do Python, a instrumentaÃ§Ã£o foi planejada com a biblioteca prometheus_client, expondo mÃ©tricas como contador de requisiÃ§Ãµes e histograma de latÃªncias; no Java, a recomendaÃ§Ã£o Ã© utilizar Micrometer com o registrador Prometheus, ativando os endpoints do actuator para exposiÃ§Ã£o das mÃ©tricas. AlÃ©m disso, os serviÃ§os devem registrar logs estruturados que incluam timestamps, endpoint, latÃªncia e status de resposta, favorecendo anÃ¡lises posteriores e correlaÃ§Ãµes entre logs e mÃ©tricas.

Os cenÃ¡rios de teste foram definidos para capturar comportamento em condiÃ§Ãµes distintas. No cenÃ¡rio ideal a rede Ã© estÃ¡vel, com latÃªncia mÃ­nima e baixa perda de pacotes; as cargas sÃ£o leves para observar o comportamento de melhor caso de cada tecnologia. No cenÃ¡rio realista introduzimos latÃªncia e pequena perda de pacotes, juntamente com cargas moderadas que simulam o comportamento em produÃ§Ã£o tÃ­pico. No cenÃ¡rio crÃ­tico aumentamos drasticamente a carga e pioramos as condiÃ§Ãµes de rede, simulando sobrecarga, picos e degradaÃ§Ã£o de conectividade para observar limites, gargalos e pontos de falha. Para simular rede degradada costuma-se usar a ferramenta netem (tc) aplicada na interface de rede do host ou dentro dos containers, permitindo parametrizar atraso, jitter e perda de pacotes de forma determinÃ­stica para cada experimento.

A coleta de mÃ©tricas cobre latÃªncia (tempo de resposta por requisiÃ§Ã£o), throughput (requisiÃ§Ãµes por segundo), consumo de CPU e memÃ³ria por serviÃ§o, escalabilidade (comportamento ao aumentar nÃºmero de clientes/conexÃµes), resiliÃªncia (capacidade de recuperaÃ§Ã£o perante falhas ou reinÃ­cios) e a facilidade de encontrar documentaÃ§Ã£o e suporte online, que Ã© um indicador qualitativo para adoÃ§Ã£o prÃ¡tica. Para execuÃ§Ã£o de carga foram escolhidas ferramentas como k6 e Apache JMeter, podendo tambÃ©m empregar bibliotecas especÃ­ficas de Python (locust) e Java para testes mais customizados; os scripts de teste devem ser versionados em /tests, com nomenclatura clara que inclua protocolo, serviÃ§o e cenÃ¡rio (por exemplo, k6_user_rest_ideal.js). Prometheus faz o scrape dos endpoints de mÃ©tricas expostos por cada serviÃ§o, e Grafana apresenta dashboards com painÃ©is de latÃªncia, throughput, uso de CPU e memÃ³ria, e taxa de erros, permitindo comparaÃ§Ã£o visual entre protocolos e cenÃ¡rios.

A padronizaÃ§Ã£o de APIs e a uniformidade dos contratos sÃ£o cruciais para que as comparaÃ§Ãµes sejam vÃ¡lidas. Para REST foi definido que a base URL Ã© /api/users e que os endpoints mÃ­nimos sÃ£o GET /users/{id}, POST /users e GET /users. As respostas seguem JSON com a mesma estrutura de campos entre implementaÃ§Ãµes. Para gRPC foi definido um arquivo user.proto com as mensagens GetUserRequest, CreateUserRequest e UserResponse, e um serviÃ§o UserService contendo rpc GetUser e rpc CreateUser. Para GraphQL foi definido um endpoint /graphql contendo query user(id: Int) e a mutation createUser(name, email). Para SOAP foi proposta uma interface WSDL simples com a operaÃ§Ã£o GetUser, e o endpoint SOAP expÃµe XML conforme o contrato. Para Webhook o servidor terÃ¡ um endpoint de registro POST /hooks/users que recebe uma callback_url; quando um evento ocorrer, o servidor envia um POST para a callback com um payload padronizado do tipo { event: "user_created", data: {...}, timestamp: ... }. Para WebSocket foi definida uma rota ws://.../ws/users com mensagens JSON de aÃ§Ãµes e eventos, por exemplo action: create_user e event: user_created. Todos esses contratos estÃ£o consolidados no arquivo gerado docs/api_contracts.md que jÃ¡ estÃ¡ no repositÃ³rio e pode servir como fonte de verdade para a implementaÃ§Ã£o.

A organizaÃ§Ã£o do repositÃ³rio segue uma hierarquia pensada para simplicidade e paralelismo de trabalho. No diretÃ³rio raiz existe docs para diagramas e documentaÃ§Ã£o, python/services que contÃ©m os serviÃ§os implementados em Python (user-service, message-service, event-service), java/services com os serviÃ§os em Java correspondentes, infra contendo docker-compose.yml e os arquivos de configuraÃ§Ã£o do Prometheus e Grafana, e tests com os scripts k6/jmeter. Cada serviÃ§o possui seu Dockerfile, seu README com instruÃ§Ãµes mÃ­nimas de execuÃ§Ã£o e um endpoint /health para verificaÃ§Ã£o simples de integridade. O docker-compose define redes e volumes se necessÃ¡rios e permite subir o ambiente completo com docker-compose up --build, ou subir serviÃ§os isolados quando se deseja testar apenas um protocolo ou serviÃ§o.

A divisÃ£o de responsabilidades foi claramente atribuÃ­da para otimizar o tempo e aproveitar experiÃªncia e preferÃªncias de cada integrante. Vitor ficarÃ¡ responsÃ¡vel pela implementaÃ§Ã£o em Python, contemplando REST com FastAPI ou Flask, gRPC com protoc/protobuf usando a biblioteca grpcio, GraphQL usando Ariadne ou Strawberry e WebSocket com o framework escolhido (FastAPI/uvicorn suporta websockets nativamente). Sanzio ficarÃ¡ responsÃ¡vel pela implementaÃ§Ã£o em Java, utilizando Spring Boot para REST e WebSocket, JAX-WS para SOAP e mecanismos de webhook/recebimento de POSTs. Ambos implementarÃ£o versÃµes REST de cada serviÃ§o para garantir comparabilidade entre linguagens, e cada implementaÃ§Ã£o deverÃ¡ expor o endpoint de mÃ©tricas prometheus compatÃ­vel para que Prometheus possa fazer o scrape. A comunicaÃ§Ã£o entre as partes Ã© sincronizada atravÃ©s do repositÃ³rio Git que contÃ©m a estrutura padrÃ£o e o histÃ³rico de mudanÃ§as, e foram definidos padrÃµes de commit e branching (branch main para release e branches de feature/xxx para desenvolvimento).

A sequÃªncia de execuÃ§Ã£o prÃ¡tica para quem entra no projeto Ã©: clonar o repositÃ³rio, abrir a pasta do projeto, ler o README principal que contÃ©m as instruÃ§Ãµes de como construir com docker-compose, carregar os diagramas em docs/diagrams para entender a arquitetura, e executar docker-compose up --build a partir da pasta infra. ApÃ³s subir os containers, verificar se os endpoints bÃ¡sicos respondem, por exemplo acessar /api/users/1 nos serviÃ§os REST, consultar /actuator/prometheus em serviÃ§os Java para checar mÃ©tricas, ou /metrics em serviÃ§os Python se for o caso. Para rodar os testes, o usuÃ¡rio deve escolher um script do diretÃ³rio tests/k6, ajustar as variÃ¡veis de host/port caso necessÃ¡rio e executar k6 run <script.js>. Os resultados do k6 podem ser exportados em JSON/CSV e, em conjunto com os dados de Prometheus, gerar painÃ©is em Grafana que jÃ¡ estarÃ£o configurados a partir dos dashboards iniciais salvados em infra/grafana (se for o caso). Para simular degradaÃ§Ã£o de rede, usa-se o comando tc netem ou executar netem dentro dos containers, aplicando delay e perda conforme a parametrizaÃ§Ã£o do cenÃ¡rio em testes/test_plan.md.

Quanto aos diagramas, a visÃ£o macro estÃ¡ representada no Diagrama de Arquitetura Geral que mostra a camada de carga (k6/JMeter), a camada de serviÃ§os (os trÃªs microserviÃ§os com as implementaÃ§Ãµes em Python e Java) e a camada de monitoramento (Prometheus/Grafana), todos orquestrados no escopo de Docker. Para exames intra-fluxo existem diagramas de sequÃªncia padronizados: o diagrama de sequÃªncia para comunicaÃ§Ã£o sÃ­ncrona descreve o fluxo Cliente â†’ Gateway/LoadTester â†’ ServiÃ§o â†’ Banco de Dados â†’ Resposta, incluindo a exportaÃ§Ã£o de mÃ©tricas ao Prometheus; o diagrama de sequÃªncia para WebSocket detalha o handshake inicial (101 Switching Protocols) e a troca contÃ­nua de mensagens; o diagrama para Webhook descreve o registro do callback e o envio de eventos assÃ­ncronos do provedor para o receptor; o diagrama para GraphQL destaca o papel do resolver e a consulta seletiva de campos. Os diagramas de classe apresentam as entidades base (User, Message, Event) e os componentes tÃ­picos dos serviÃ§os (Controller, Service, Repository) para deixar claro como a lÃ³gica estÃ¡ organizada em ambas linguagens. Finalmente, os diagramas comparativos e o fluxograma de decisÃ£o serÃ£o produzidos a partir dos resultados para consolidar recomendaÃ§Ãµes prÃ¡ticas.

A estratÃ©gia de testes contempla, para cada combinaÃ§Ã£o protocolo Ã— serviÃ§o, uma bateria de experimentos repetida para cenÃ¡rios ideal, realista e crÃ­tico. Cada experimento registra mÃ©tricas primÃ¡rias e secundÃ¡rias e salva seus dados de saÃ­da em um diretÃ³rio organizado por data, protocolo e cenÃ¡rio, por exemplo tests/results/k6_rest_user_ideal_20251101.json. Os dados coletados sÃ£o tratados com scripts simples em Python (pandas) quando for necessÃ¡rio agregar e produzir as tabelas e grÃ¡ficos que irÃ£o constar no relatÃ³rio. A apresentaÃ§Ã£o final deve incluir painÃ©is Grafana selecionados, tabelas comparativas claras e um fluxograma de decisÃ£o que sintetize os trade-offs observados. O cronograma geral atÃ© o dia da apresentaÃ§Ã£o 13/11 foi definido em marcos semanais, com a semana inicial (24/10â€“30/10) dedicada Ã  estruturaÃ§Ã£o e preparaÃ§Ã£o do projeto, a segunda semana ao desenvolvimento dos serviÃ§os, a terceira Ã  execuÃ§Ã£o extensiva dos testes e a Ãºltima Ã  anÃ¡lise dos resultados e preparaÃ§Ã£o dos slides. Para a semana em curso, hÃ¡ instruÃ§Ãµes detalhadas dia a dia que orientam desde a criaÃ§Ã£o do repositÃ³rio atÃ© o primeiro docker-compose com Prometheus raspando mÃ©tricas, e a estratÃ©gia Ã© priorizar primeiro a infraestrutura que permita rodar testes simples, depois instrumentar mais profundamente e sÃ³ entÃ£o partir para cenÃ¡rios crÃ­ticos.

Para um novo integrante que entra no projeto Ã© recomendÃ¡vel comeÃ§ar lendo a documentaÃ§Ã£o em docs, clonando o repositÃ³rio e subindo o ambiente com docker-compose. Em seguida deve abrir os diagramas da pasta docs/diagrams para ter a visÃ£o global, conferir o arquivo docs/api_contracts.md para conhecer os contratos mÃ­nimos e rodar um teste k6 simples sobre o serviÃ§o REST de referÃªncia para validar que o fluxo de testes estÃ¡ funcionando. ApÃ³s entender essa sequÃªncia, o prÃ³ximo passo Ã© instrumentar mÃ©tricas adicionais ou implementar um dos protocolos faltantes em linguagem de preferÃªncia, sempre seguindo os contratos e os padrÃµes de logging definidos no repositÃ³rio. Qualquer mudanÃ§a de contrato deve ser negociada e versionada no docs/api_contracts.md e nas pastas de serviÃ§os correspondentes, e novas mÃ©tricas ou dashboards em Grafana devem ser adicionados sob infra/grafana para manter rastreabilidade



#TODO

Fazer documentaÃ§Ã£o 
Salvar explicaÃ§Ãµes
Entender 100% de tudo
Dia 6
Dia 7 -->


# ğŸ”— Comparative Study of Communication Protocols and Standards

Este projeto tem como objetivo realizar uma **anÃ¡lise comparativa entre seis protocolos e padrÃµes de comunicaÃ§Ã£o** amplamente utilizados em sistemas distribuÃ­dos: **REST**, **SOAP**, **gRPC**, **WebSocket**, **GraphQL** e **Webhook**.  
A proposta Ã© avaliar o desempenho, escalabilidade, resiliÃªncia e consumo de recursos de cada tecnologia sob diferentes condiÃ§Ãµes de rede, auxiliando na escolha ideal de comunicaÃ§Ã£o entre sistemas.

---

## ğŸ§­ Estrutura do Projeto

O projeto foi desenvolvido de forma modular, separando os serviÃ§os por linguagem e funÃ§Ã£o, garantindo flexibilidade e independÃªncia entre os componentes. A infraestrutura completa Ã© orquestrada via **Docker Compose**.

```
ğŸ“‚ project-root/
â”œâ”€â”€ python/
â”‚   â””â”€â”€ user-service/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ java/
â”‚   â””â”€â”€ user-service/
â”‚       â”œâ”€â”€ src/
â”‚       â”œâ”€â”€ pom.xml
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api_contracts.md
â”‚   â””â”€â”€ diagrams/
â””â”€â”€ README.md
```

---

## âš™ï¸ Tecnologias Utilizadas

### **Linguagens e Frameworks**
- **Python (FastAPI)** â€“ utilizado para construir serviÃ§os leves e rÃ¡pidos, com fÃ¡cil integraÃ§Ã£o a Prometheus.
- **Java (Spring Boot)** â€“ escolhido para sua robustez e uso comum em sistemas corporativos.
- **gRPC / GraphQL / WebSocket / Webhook** â€“ implementados sobre essas bases para comparaÃ§Ã£o direta de desempenho.

### **Infraestrutura e Monitoramento**
- **Docker / Docker Compose** â€“ garante isolamento e reprodutibilidade do ambiente de testes.
- **Prometheus** â€“ coleta mÃ©tricas de desempenho (latÃªncia, throughput, uso de CPU/memÃ³ria).
- **Grafana** â€“ visualiza as mÃ©tricas coletadas em dashboards dinÃ¢micos.
- **k6 / Apache JMeter** â€“ ferramentas utilizadas para simular carga e medir o comportamento dos serviÃ§os.

---

## ğŸ§© Funcionamento da Infraestrutura

A arquitetura foi planejada para permitir testes padronizados entre linguagens e protocolos.  
Cada serviÃ§o (tanto em Python quanto em Java) expÃµe um endpoint `/users/{id}`, com a mesma resposta em JSON, garantindo a equivalÃªncia funcional durante as mediÃ§Ãµes.

O **Prometheus** Ã© configurado para â€œrasparâ€ os endpoints de mÃ©tricas expostos por cada serviÃ§o, permitindo registrar informaÃ§Ãµes de uso e desempenho.  
Esses dados podem ser visualizados no **Grafana**, que exibe os grÃ¡ficos comparativos em tempo real.

### Fluxo geral:
1. O Docker Compose sobe todos os containers (`python_service`, `java_service`, `prometheus`, e opcionalmente `grafana`).
2. O Prometheus comeÃ§a a monitorar os endpoints de mÃ©tricas definidos.
3. Os serviÃ§os sÃ£o testados com k6, JMeter ou scripts Python/Java customizados.
4. As mÃ©tricas sÃ£o salvas e comparadas entre os diferentes protocolos e padrÃµes.

---

## ğŸ§± ConfiguraÃ§Ã£o dos ServiÃ§os

### Python (FastAPI)
Arquivo `main.py`:

```python
from fastapi import FastAPI
from prometheus_client import start_http_server, Counter

app = FastAPI()
REQUESTS = Counter('requests_total', 'Total requests')
start_http_server(8001)

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    REQUESTS.inc()
    return {"id": user_id, "name": f"User{user_id}", "email": "user@example.com"}
```

Arquivo `requirements.txt`:

```
fastapi
uvicorn[standard]
prometheus-client
```

Arquivo `Dockerfile`:

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

### Java (Spring Boot)
Arquivo `Dockerfile`:

```dockerfile
FROM maven:3.8-openjdk-17 AS build
WORKDIR /app
COPY pom.xml .
COPY src ./src
RUN mvn -q -DskipTests package

FROM openjdk:17-jdk-slim
COPY --from=build /app/target/app.jar /app/app.jar
CMD ["java","-jar","/app/app.jar"]
```

---

## ğŸ§  Observabilidade

Arquivo `infra/prometheus/prometheus.yml`:

```yaml
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'python_service'
    static_configs:
      - targets: ['python_service:8001']

  - job_name: 'java_service'
    static_configs:
      - targets: ['java_service:8080']
```

Arquivo `infra/docker-compose.yml`:

```yaml
version: '3.8'
services:
  python_service:
    build: ../python/user-service
    ports:
      - "8000:8000"
    networks:
      - testnet

  java_service:
    build: ../java/user-service
    ports:
      - "8080:8080"
    networks:
      - testnet

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - testnet

networks:
  testnet:
```

---

## ğŸ”¬ Testes e MÃ©tricas

Os testes sÃ£o conduzidos em trÃªs cenÃ¡rios distintos:

- **Ideal:** ambiente estÃ¡vel e com baixa latÃªncia.
- **Realista:** condiÃ§Ãµes tÃ­picas de rede com variaÃ§Ãµes moderadas.
- **CrÃ­tico:** alta carga, perda de pacotes e concorrÃªncia elevada.

As mÃ©tricas avaliadas incluem:
- LatÃªncia (tempo de resposta)
- Throughput (requisiÃ§Ãµes por segundo)
- Uso de CPU e memÃ³ria
- Escalabilidade e resiliÃªncia

---

## ğŸ§¾ HistÃ³rico de Comandos e Etapas Executadas

### Dia 4 â€” 27/10: ConfiguraÃ§Ã£o inicial dos serviÃ§os
```bash
cd python/user-service
pip install -r requirements.txt
uvicorn main:app --reload
# Teste local
curl http://localhost:8000/users/1
```

### Dia 5 â€” 28/10: Infraestrutura com Docker Compose e Prometheus
```bash
# Na pasta infra/
docker-compose up --build
# VerificaÃ§Ã£o
curl http://localhost:8000/users/1
curl http://localhost:8080/users/1
# Acesso ao Prometheus
http://localhost:9090
```

---

## ğŸ“ˆ PrÃ³ximos Passos

1. Adicionar instrumentaÃ§Ã£o completa de mÃ©tricas aos demais protocolos.
2. Integrar Grafana para dashboards de desempenho.
3. Executar os testes comparativos em cenÃ¡rios ideal, realista e crÃ­tico.
4. Gerar tabelas e grÃ¡ficos de anÃ¡lise.

---

## ğŸ‘¥ Autores

**Vitor Lopes Nocce** â€“ ImplementaÃ§Ã£o Python, instrumentaÃ§Ã£o e anÃ¡lise de mÃ©tricas.  
**Rafael Sanzio e Silva** â€“ ImplementaÃ§Ã£o Java, integraÃ§Ã£o e monitoramento.

---

## ğŸ§© LicenÃ§a

Este projeto Ã© de carÃ¡ter **acadÃªmico e experimental**, desenvolvido para fins de pesquisa no contexto de anÃ¡lise de protocolos de comunicaÃ§Ã£o em sistemas distribuÃ­dos.
